{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357756be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input number of threads: 1\n",
      "crawling:  https://e-prostor.gov.si/\n",
      "Data for e-prostor.gov.si saved successfully.\n",
      "site_id: 5\n",
      "Error fetching https://e-prostor.gov.si/: Timeout value connect was <object object at 0x7fbc07ef0ae0>, but it must be an int, float or None.\n",
      "Cannot unpack html and status code, probably, didn't receive a response\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 410\u001b[0m\n\u001b[1;32m    408\u001b[0m n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease input number of threads: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    409\u001b[0m num_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_threads)\n\u001b[0;32m--> 410\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 333\u001b[0m, in \u001b[0;36mcrawl\u001b[0;34m()\u001b[0m\n\u001b[1;32m    329\u001b[0m cur\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDELETE FROM frontier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWHERE id = (SELECT id FROM frontier ORDER BY id LIMIT 1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURNING link\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    332\u001b[0m row \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mfetchone()\n\u001b[0;32m--> 333\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    334\u001b[0m current_url \u001b[38;5;241m=\u001b[39m url\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrawling: \u001b[39m\u001b[38;5;124m\"\u001b[39m, url)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import hashlib\n",
    "import psycopg2\n",
    "import io\n",
    "import PyPDF2\n",
    "import docx\n",
    "import pptx\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "import base64\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import sys\n",
    "\n",
    "# Define a lock for thread-safe access to shared resources\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "# Function to retrieve and parse HTML content using requests\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        firefox_options = Options()\n",
    "        firefox_options.add_argument(\"-headless\")\n",
    "        driver = webdriver.Firefox(options=firefox_options)\n",
    "        driver.get(url)\n",
    "        response = requests.head(url, timeout=5)\n",
    "        status_code = response.status_code\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "        return html, status_code\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to extract links from HTML content\n",
    "def extract_links(html, base_url):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = set()\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href')\n",
    "        if href.startswith('http') or href.startswith('https'):\n",
    "            links.add(href)\n",
    "        else:\n",
    "            links.add(urljoin(base_url, href))\n",
    "    return links\n",
    "\n",
    "\n",
    "# Function to respect crawl delay\n",
    "def respect_crawl_delay(domain, delay=5):\n",
    "    current_time = time.time()\n",
    "    last_access_time = last_access_times.get(domain, 0)\n",
    "    if current_time - last_access_time < delay:\n",
    "        time.sleep(delay - (current_time - last_access_time))\n",
    "    last_access_times[domain] = time.time()\n",
    "\n",
    "\n",
    "# Function to canonicalize URLs\n",
    "def canonicalize_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    # Scheme and netloc are converted to lowercase\n",
    "    scheme = parsed_url.scheme.lower()\n",
    "    netloc = parsed_url.netloc.lower()\n",
    "    # Path is stripped of trailing slashes and converted to lowercase\n",
    "    path = parsed_url.path.rstrip('/').lower()\n",
    "    # Query and fragment are ignored\n",
    "    return f\"{scheme}://{netloc}{path}\"\n",
    "\n",
    "\n",
    "# Function to crawl a single URL\n",
    "def crawl_url(url, site_id):\n",
    "    global current_url\n",
    "    html, status_code = None, None\n",
    "    try:\n",
    "        html, status_code = fetch_html(url)\n",
    "    except TypeError:\n",
    "        print(\"Cannot unpack html and status code, probably, didn't receive a response\")\n",
    "    if url not in visited_urls:\n",
    "        if html is not None and status_code is not None:  # Store canonicalized URL\n",
    "            print(\"crawling: \" + url)\n",
    "            canonical_url = canonicalize_url(url)\n",
    "            print(\"canonicalized: \" + canonical_url)\n",
    "            # Check for duplicate content\n",
    "            if (not is_duplicate_html(html)) and (not is_duplicate_url(url)):\n",
    "                # Extract links\n",
    "                links = extract_links(html, url)\n",
    "                for link in links:\n",
    "                    # print(\"link: \" + link)\n",
    "                    cur.execute(\"INSERT INTO frontier (link) VALUES (%s)\", (link, ))\n",
    "                # Store data in the database\n",
    "                store_data(url, canonical_url, html, status_code, site_id)\n",
    "            else:\n",
    "                store_data(url, canonical_url, html, status_code, site_id)\n",
    "                cur.execute(\"SELECT id FROM page WHERE url = %s\", (canonical_url,))\n",
    "                from_id = cur.fetchone()[0]\n",
    "                cur.execute(\"SELECT id FROM page WHERE url = %s\", (canonicalize_url(current_url),))\n",
    "                to_id = cur.fetchone()[0]\n",
    "                cur.execute(\"INSERT INTO link (from_page, to_page) VALUES (%s, %s)\",\n",
    "                            (from_id, to_id))\n",
    "                conn.commit()\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Function to check for duplicate content\n",
    "def is_duplicate_html(html):\n",
    "    # Check if the hash of the HTML content already exists in the database\n",
    "    html_hash = hashlib.sha256(html.encode()).hexdigest()\n",
    "    cur.execute(\"SELECT * FROM page WHERE html_content_hash = %s\", (html_hash,))\n",
    "    return cur.fetchone() is not None\n",
    "\n",
    "\n",
    "def is_duplicate_url(url):\n",
    "    canonical_url = canonicalize_url(url)\n",
    "    cur.execute(\"SELECT * FROM page WHERE url = %s\", (canonical_url,))\n",
    "    return cur.fetchone() is not None\n",
    "\n",
    "\n",
    "# Function to store data in the database\n",
    "def store_data(url, canonical_url, html, status_code, site_id):\n",
    "    page_types = [\"HTML\", \"BINARY\", \"DUPLICATE\", \"FRONTIER\"]\n",
    "    if '.pdf' in url or '.doc' in url or '.docx' in url or '.ppt' in url or '.pptx' in url:\n",
    "        html_hash = hashlib.sha256(html.encode()).hexdigest()\n",
    "        cur.execute(\"INSERT INTO page (site_id, page_type_code, url, html_content_hash, html_content, http_status_code, accessed_time) \"\n",
    "                    \"VALUES (%s, %s, %s, %s, %s, %s, %s) RETURNING id\",\n",
    "                    (site_id, page_types[1], canonical_url, html_hash, html, status_code, datetime.now()))\n",
    "        page_id = cur.fetchone()[0]  # Get the inserted page ID\n",
    "        conn.commit()\n",
    "    else:\n",
    "        html_hash = hashlib.sha256(html.encode()).hexdigest()\n",
    "        cur.execute(\"INSERT INTO page (site_id, page_type_code, url, html_content_hash, html_content, http_status_code, accessed_time) \"\n",
    "                    \"VALUES (%s, %s, %s, %s, %s, %s, %s) RETURNING id\",\n",
    "                    (site_id, page_types[0], canonical_url, html_hash, html, status_code, datetime.now()))\n",
    "        page_id = cur.fetchone()[0]  # Get the inserted page ID\n",
    "        conn.commit()\n",
    "\n",
    "    # Check for specific file extensions in the URL and store them in a separate table\n",
    "    file_extensions = ['.pdf', '.doc', '.docx', '.ppt', '.pptx']\n",
    "    for ext in file_extensions:\n",
    "        if ext in url.lower():  # Check if the URL contains the file extension\n",
    "            file_type = ext.upper()  # Extract file type\n",
    "            file_content = None\n",
    "            if file_type == '.pdf':\n",
    "                file_content = extract_pdf_content(canonical_url)\n",
    "            elif file_type in ['.doc', '.docx']:\n",
    "                file_content = extract_doc_content(canonical_url)\n",
    "            elif file_type in ['.ppt', '.pptx']:\n",
    "                file_content = extract_ppt_content(canonical_url)\n",
    "\n",
    "            # Insert the file content into the page_data table\n",
    "            if file_content:\n",
    "                cur.execute(\"INSERT INTO page_data (page_id, data_type_code, data) VALUES (%s, %s, %s)\",\n",
    "                            (page_id, ext.upper(), file_content))\n",
    "                conn.commit()\n",
    "\n",
    "    images = extract_images(url, html)\n",
    "    for image_url, image_content in images.items():\n",
    "        save_image(page_id, image_url, image_content)\n",
    "\n",
    "\n",
    "# Function to extract content from a PDF file\n",
    "def extract_pdf_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        pdf_file = PyPDF2.PdfFileReader(io.BytesIO(response.content))\n",
    "        text = \"\"\n",
    "        for page_num in range(pdf_file.numPages):\n",
    "            text += pdf_file.getPage(page_num).extractText()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting PDF content from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to extract content from a DOC or DOCX file\n",
    "def extract_doc_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        doc = docx.Document(io.BytesIO(response.content))\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting DOC/DOCX content from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to extract content from a PPT or PPTX file\n",
    "def extract_ppt_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        ppt = pptx.Presentation(io.BytesIO(response.content))\n",
    "        text = \"\"\n",
    "        for slide in ppt.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text += shape.text\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting PPT/PPTX content from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_images(site_url, html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    images = {}\n",
    "    for img_tag in soup.find_all('img'):\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            img_content = fetch_image(site_url, img_url)\n",
    "            if img_content:\n",
    "                images[img_url] = img_content\n",
    "    return images\n",
    "\n",
    "\n",
    "def fetch_image(site_url, image_url):\n",
    "    try:\n",
    "        if image_url.startswith('data:image'):\n",
    "            # Extract base64 data from the URL\n",
    "            base64_data = image_url.split(',')[1]\n",
    "            # Decode base64 data\n",
    "            image_content = base64.b64decode(base64_data)\n",
    "            return image_content\n",
    "        elif not urlparse(image_url).scheme:\n",
    "            image_url = site_url + image_url  # Assuming HTTPS, you can adjust as needed\n",
    "            response = requests.get(image_url)\n",
    "            if response.status_code == 200:\n",
    "                return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching image from {image_url}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_image(page_id, url, content):\n",
    "    filename = url.split('/')[-1]  # Extract filename from URL\n",
    "    content_type = 'image/jpeg'  # Adjust content type based on image format if needed\n",
    "    try:\n",
    "        cur.execute(\"INSERT INTO image (page_id, filename, content_type, data, accessed_time) \"\n",
    "                    \"VALUES (%s, %s, %s, %s, %s)\",\n",
    "                    (page_id, filename, content_type, content, datetime.now()))\n",
    "        conn.commit()\n",
    "        print(f\"Image saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image {filename}: {e}\")\n",
    "\n",
    "\n",
    "# Function to fetch and parse the robots.txt file\n",
    "def parse_robots_txt(domain):\n",
    "    robots_txt_url = urljoin(domain, \"/robots.txt\")\n",
    "    try:\n",
    "        response = requests.get(robots_txt_url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {robots_txt_url}: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Function to parse robots.txt rules\n",
    "def parse_robots_rules(robots_txt_content):\n",
    "    rules = {}\n",
    "    current_user_agent = None\n",
    "    for line in robots_txt_content.splitlines():\n",
    "        if line.strip():  # Ignore empty lines\n",
    "            if line.lower().startswith(\"user-agent\"):\n",
    "                current_user_agent = line.split(\":\")[1].strip()\n",
    "            elif current_user_agent and \":\" in line:  # Check if line contains a colon\n",
    "                key, value = line.split(\":\", 1)\n",
    "                rules.setdefault(current_user_agent, []).append((key.strip(), value.strip()))\n",
    "    return rules\n",
    "\n",
    "\n",
    "# Function to check if a URL is allowed to be crawled\n",
    "def is_allowed_by_robots(url, robots_rules):\n",
    "    parsed_url = urlparse(url)\n",
    "    user_agent = \"*\"\n",
    "    if \"User-agent\" in robots_rules:\n",
    "        user_agent = robots_rules[\"User-agent\"]\n",
    "    elif \"*\" in robots_rules:\n",
    "        user_agent = \"*\"\n",
    "    if user_agent in robots_rules:\n",
    "        for rule_key, rule_value in robots_rules[user_agent]:\n",
    "            if rule_key.lower() == \"allow\" and rule_value != \"/\" and parsed_url.path.startswith(rule_value):\n",
    "                return True\n",
    "            elif rule_key.lower() == \"disallow\" and parsed_url.path.startswith(rule_value):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Function to fetch and parse the sitemap.xml file\n",
    "def parse_sitemap_xml(domain):\n",
    "    sitemap_url = urljoin(domain, \"/sitemap.xml\")\n",
    "    try:\n",
    "        response = requests.get(sitemap_url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {sitemap_url}: {e}\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Function to save data into the \"site\" table\n",
    "def save_site_data(domain, robots_content, sitemap_content):\n",
    "    try:\n",
    "        cur.execute(\"INSERT INTO site (domain, robots_content, sitemap_content) VALUES (%s, %s, %s) \"\n",
    "                    \"RETURNING id\",\n",
    "                   (domain, robots_content, sitemap_content))\n",
    "        id = cur.fetchone()[0]\n",
    "        conn.commit()\n",
    "        print(f\"Data for {domain} saved successfully.\")\n",
    "        return id\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data for {domain}: {e}\")\n",
    "\n",
    "\n",
    "# Function to crawl websites\n",
    "def crawl():\n",
    "    global current_url\n",
    "    while True:\n",
    "        url = None\n",
    "        # Ensure thread safety while accessing shared frontier\n",
    "        cur.execute(\"DELETE FROM frontier \"\n",
    "                    \"WHERE id = (SELECT id FROM frontier ORDER BY id LIMIT 1) \"\n",
    "                    \"RETURNING link\")\n",
    "        row = cur.fetchone()\n",
    "        url = row[0]\n",
    "        current_url = url\n",
    "        print(\"crawling: \", url)\n",
    "        if url:\n",
    "            domain = urlparse(url).netloc\n",
    "            respect_crawl_delay(domain)\n",
    "            robots = parse_robots_txt(url)\n",
    "            rules = parse_robots_rules(robots)\n",
    "            if is_allowed_by_robots(url, rules):\n",
    "                sitemap = parse_sitemap_xml(url)\n",
    "                site_id = save_site_data(domain, robots, sitemap)\n",
    "                cur.execute(\"SELECT id FROM site WHERE domain = %s\", (domain, ))\n",
    "                print(\"site_id: \" + str(site_id))\n",
    "                crawl_url(url, site_id)\n",
    "                visited_urls.add(url)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "# Initialize database connection\n",
    "conn = psycopg2.connect(database=\"postgres\", user=\"postgres.guoimnempzxzidvwjnem\", password=\"IepsCrawler12!!\",\n",
    "                        host=\"aws-0-eu-west-2.pooler.supabase.com\", port=\"5432\")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "global visited_urls\n",
    "visited_urls = set()\n",
    "\n",
    "# Global variable to store last access times for domains\n",
    "global last_access_times\n",
    "last_access_times = {}\n",
    "\n",
    "global current_url\n",
    "\n",
    "# Number of threads to use\n",
    "num_threads = 5  # Adjust as needed\n",
    "\n",
    "# Queue to hold URLs to be processed\n",
    "url_queue = Queue()\n",
    "\n",
    "\n",
    "# Worker function for each thread\n",
    "def worker():\n",
    "    while True:\n",
    "        url = url_queue.get()\n",
    "        if url is None:\n",
    "            break\n",
    "        crawl_url(url)\n",
    "        url_queue.task_done()\n",
    "\n",
    "\n",
    "# Create and start threads\n",
    "threads = []\n",
    "for _ in range(num_threads):\n",
    "    t = threading.Thread(target=worker)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# Add URLs to the queue\n",
    "# Example: url_queue.put('https://example.com')\n",
    "# Add your URLs accordingly\n",
    "\n",
    "# Wait for all threads to finish\n",
    "url_queue.join()\n",
    "\n",
    "# Stop worker threads\n",
    "for _ in range(num_threads):\n",
    "    url_queue.put(None)\n",
    "\n",
    "# Join all threads\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_threads = input(\"Please input number of threads: \")\n",
    "    num_threads = int(n_threads)\n",
    "    crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d8bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
